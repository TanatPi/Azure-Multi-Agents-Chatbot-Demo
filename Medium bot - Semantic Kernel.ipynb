{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96700e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings,\n",
    ")\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "endpoint = os.environ.get('AZURE_OPENAI_RESOURCE')\n",
    "model_name = os.environ.get('AZURE_OPENAI_MODEL')\n",
    "deployment = os.environ.get('AZURE_OPENAI_MODEL')\n",
    "subscription_key = os.environ.get('AZURE_OPENAI_KEY')\n",
    "\n",
    "# Initialize the kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add Azure OpenAI chat completion\n",
    "chat_completion = AzureChatCompletion(\n",
    "    deployment_name=deployment,\n",
    "    api_key=subscription_key,\n",
    "    endpoint=endpoint,\n",
    ")\n",
    "kernel.add_service(chat_completion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce0b94db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Get root logger\n",
    "root_logger = logging.getLogger(\"kernel\")\n",
    "root_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Remove all existing handlers\n",
    "for handler in root_logger.handlers[:]:\n",
    "    root_logger.removeHandler(handler)\n",
    "\n",
    "# Create file handler to overwrite the log file\n",
    "f_handler = logging.FileHandler('my_log.log', mode='w')\n",
    "f_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Formatter\n",
    "formatter = logging.Formatter('[%(asctime)s - %(name)s:%(lineno)d - %(levelname)s] %(message)s',\n",
    "                              datefmt='%Y-%m-%d %H:%M:%S')\n",
    "f_handler.setFormatter(formatter)\n",
    "\n",
    "# Add file handler only to root logger\n",
    "root_logger.addHandler(f_handler)\n",
    "\n",
    "# Optionally adjust specific logger levels, e.g., semantic kernel logger\n",
    "logging.getLogger(\"kernel\").setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"kernel\").propagate = True  # allow logs to bubble to root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11d593f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion : AzureChatCompletion = kernel.get_service(type=ChatCompletionClientBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ef227b",
   "metadata": {},
   "source": [
    "Sample plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68a10310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "class LightsPlugin:\n",
    "    lights = [\n",
    "        {\"id\": 1, \"name\": \"Table Lamp\", \"is_on\": False},\n",
    "        {\"id\": 2, \"name\": \"Porch light\", \"is_on\": False},\n",
    "        {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": True},\n",
    "    ]\n",
    "\n",
    "    @kernel_function(\n",
    "        name=\"get_lights\",\n",
    "        description=\"Gets a list of lights and their current state\",\n",
    "    )\n",
    "    def get_state(\n",
    "        self,\n",
    "    ) -> str:\n",
    "        \"\"\"Gets a list of lights and their current state.\"\"\"\n",
    "        return self.lights\n",
    "\n",
    "    @kernel_function(\n",
    "        name=\"change_state\",\n",
    "        description=\"Changes the state of the light\",\n",
    "    )\n",
    "    def change_state(\n",
    "        self,\n",
    "        id: int,\n",
    "        is_on: bool,\n",
    "    ) -> str:\n",
    "        \"\"\"Changes the state of the light.\"\"\"\n",
    "        for light in self.lights:\n",
    "            if light[\"id\"] == id:\n",
    "                light[\"is_on\"] = is_on\n",
    "                return light\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f0ca4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='Lights', description=None, functions={'change_state': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='change_state', plugin_name='Lights', description='Changes the state of the light', parameters=[KernelParameterMetadata(name='id', description=None, default_value=None, type_='int', is_required=True, type_object=<class 'int'>, schema_data={'type': 'integer'}, include_in_function_choices=True), KernelParameterMetadata(name='is_on', description=None, default_value=None, type_='bool', is_required=True, type_object=<class 'bool'>, schema_data={'type': 'boolean'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=False, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000001F3A6A37A90>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000001F3A6AA6810>, method=<bound method LightsPlugin.change_state of <__main__.LightsPlugin object at 0x000001F3A6AA6A50>>, stream_method=None), 'get_lights': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_lights', plugin_name='Lights', description='Gets a list of lights and their current state', parameters=[], is_prompt=False, is_asynchronous=False, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000001F3A6ADF4D0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000001F3A68CF5D0>, method=<bound method LightsPlugin.get_state of <__main__.LightsPlugin object at 0x000001F3A6AA6A50>>, stream_method=None)})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the plugin to the kernel\n",
    "kernel.add_plugin(\n",
    "    LightsPlugin(),\n",
    "    plugin_name=\"Lights\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4010e7",
   "metadata": {},
   "source": [
    "Auto agentic behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf60b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e8eefb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > Hello! How can I help you today?\n",
      "Exiting chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Initiate a back-and-forth chat\n",
    "userInput = None\n",
    "\n",
    "# Create a history of the conversation\n",
    "history = ChatHistory()\n",
    "\n",
    "exit_commands = {\"exit\", \"quit\", \"bye\"}\n",
    "while True:\n",
    "    # Collect user input\n",
    "    userInput = input(\"User > \").strip()\n",
    "\n",
    "    if userInput.lower() in exit_commands:\n",
    "        print(\"Exiting chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Add user input to the history\n",
    "    history.add_user_message(userInput)\n",
    "\n",
    "    # Get the response from the AI\n",
    "    result = await chat_completion.get_chat_message_content(\n",
    "        chat_history=history,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Assistant > \" + str(result))\n",
    "\n",
    "    # Add the message from the agent to the chat history\n",
    "    history.add_message(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
